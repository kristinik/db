name: Lab33

on:
  push:
    branches: ["main"]

env:
  AWS_REGION: eu-central-1

  # RDS
  RDS_ID: iotlab-db
  DB_NAME: iotlab
  DB_USER: admin
  RDS_SG_NAME: iotlab-rds-sg
  LAMBDA_SG_NAME: iotlab-lambda-sg

  # SQS + Lambdas + API
  SQS_NAME: iotlab-queue
  INGEST_LAMBDA: iotlab-ingest
  CONSUMER_LAMBDA: iotlab-consumer
  LAMBDA_ROLE: iotlab-lambda-role
  API_NAME: iotlab-api
  STAGE: prod

  # Emulator via ECS Fargate
  ECR_REPOSITORY: iotlab-emulator
  CLUSTER_NAME: iotlab-cluster
  EMU_TASK_FAMILY: iotlab-emulator-task
  EMU_CONTAINER: iotlab-emulator
  EMU_CPU: "256"
  EMU_MEM: "512"

  # Частоти сенсорів (мс)
  TEMP_PERIOD_MS: "50"
  HUM_PERIOD_MS: "80"
  LIGHT_PERIOD_MS: "100"

jobs:
  deploy-iot-lab:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Generate strong DB password (no extra GitHub secret)
        id: genpass
        run: |
          set -euo pipefail
          DB_MASTER_PASS=$(python3 - <<'PY'
          import secrets, string
          alphabet = string.ascii_letters + string.digits + "-_@%+."
          print(''.join(secrets.choice(alphabet) for _ in range(24)))
          PY
          )
          echo "::add-mask::$DB_MASTER_PASS"
          echo "DB_MASTER_PASS=$DB_MASTER_PASS" >> $GITHUB_ENV
      - name: Resolve VPC + subnets
        id: vpc
        run: |
          set -e
          VPC_ID=$(aws ec2 describe-vpcs --filters Name=isDefault,Values=true --query 'Vpcs[0].VpcId' --output text)
          SUBNETS=$(aws ec2 describe-subnets --filters Name=vpc-id,Values=$VPC_ID --query 'Subnets[?MapPublicIpOnLaunch==`true`].SubnetId' --output text)
          SUBNET1=$(echo $SUBNETS | awk '{print $1}'); SUBNET2=$(echo $SUBNETS | awk '{print $2}')
          if [ -z "$SUBNET1" ] || [ -z "$SUBNET2" ]; then echo "Need >=2 public subnets in default VPC"; exit 1; fi
          echo "VPC_ID=$VPC_ID"       >> $GITHUB_ENV
          echo "SUBNET1=$SUBNET1"     >> $GITHUB_ENV
          echo "SUBNET2=$SUBNET2"     >> $GITHUB_ENV
      - name: Security groups (RDS + Lambda)
        run: |
          set -e
          # Lambda SG (egress all)
          LAMBDA_SG_ID=$(aws ec2 describe-security-groups --filters Name=group-name,Values="$LAMBDA_SG_NAME" Name=vpc-id,Values="$VPC_ID" --query 'SecurityGroups[0].GroupId' --output text)
          [ "$LAMBDA_SG_ID" = "None" ] || [ -z "$LAMBDA_SG_ID" ] && \
            LAMBDA_SG_ID=$(aws ec2 create-security-group --group-name "$LAMBDA_SG_NAME" --description "IoT Lab Lambda SG" --vpc-id "$VPC_ID" --query 'GroupId' --output text)
          aws ec2 revoke-security-group-egress --group-id "$LAMBDA_SG_ID" --ip-permissions '[{"IpProtocol":"-1","IpRanges":[{"CidrIp":"0.0.0.0/0"}]}]' >/dev/null 2>&1 || true
          aws ec2 authorize-security-group-egress --group-id "$LAMBDA_SG_ID" --ip-permissions '[{"IpProtocol":"-1","IpRanges":[{"CidrIp":"0.0.0.0/0"}]}]' >/dev/null 2>&1 || true
          # RDS SG (ingress 3306 only from Lambda SG)
          RDS_SG_ID=$(aws ec2 describe-security-groups --filters Name=group-name,Values="$RDS_SG_NAME" Name=vpc-id,Values="$VPC_ID" --query 'SecurityGroups[0].GroupId' --output text)
          [ "$RDS_SG_ID" = "None" ] || [ -z "$RDS_SG_ID" ] && \
            RDS_SG_ID=$(aws ec2 create-security-group --group-name "$RDS_SG_NAME" --description "IoT Lab RDS SG" --vpc-id "$VPC_ID" --query 'GroupId' --output text)
          aws ec2 authorize-security-group-ingress --group-id "$RDS_SG_ID" --protocol tcp --port 3306 --source-group "$LAMBDA_SG_ID" >/dev/null 2>&1 || true
          echo "LAMBDA_SG_ID=$LAMBDA_SG_ID" >> $GITHUB_ENV
          echo "RDS_SG_ID=$RDS_SG_ID"       >> $GITHUB_ENV
      - name: Create RDS MySQL (private)
        run: |
          set -e
          if ! aws rds describe-db-instances --db-instance-identifier "$RDS_ID" >/dev/null 2>&1; then
            aws rds create-db-instance \
              --db-instance-identifier "$RDS_ID" \
              --engine mysql --engine-version 8.0.35 \
              --db-instance-class db.t4g.micro \
              --allocated-storage 20 \
              --master-username "$DB_USER" \
              --master-user-password "$DB_MASTER_PASS" \
              --vpc-security-group-ids "$RDS_SG_ID" \
              --db-name "$DB_NAME" \
              --no-multi-az --publicly-accessible false \
              --backup-retention-period 0 \
              --storage-type gp2
          fi
          aws rds wait db-instance-available --db-instance-identifier "$RDS_ID"
          DB_HOST=$(aws rds describe-db-instances --db-instance-identifier "$RDS_ID" --query 'DBInstances[0].Endpoint.Address' --output text)
          echo "DB_HOST=$DB_HOST" >> $GITHUB_ENV
          echo "RDS endpoint: $DB_HOST"
      - name: Create SQS queue
        id: sqs
        run: |
          set -e
          aws sqs create-queue --queue-name "$SQS_NAME" >/dev/null 2>&1 || true
          QUEUE_URL=$(aws sqs get-queue-url --queue-name "$SQS_NAME" --query 'QueueUrl' --output text)
          QUEUE_ARN=$(aws sqs get-queue-attributes --queue-url "$QUEUE_URL" --attribute-names QueueArn --query 'Attributes.QueueArn' --output text)
          echo "QUEUE_URL=$QUEUE_URL" >> $GITHUB_ENV
          echo "QUEUE_ARN=$QUEUE_ARN" >> $GITHUB_ENV
          echo "Queue: $QUEUE_URL"
      - name: IAM role for Lambdas
        run: |
          set -e
          TRUST='{"Version":"2012-10-17","Statement":[{"Effect":"Allow","Principal":{"Service":"lambda.amazonaws.com"},"Action":"sts:AssumeRole"}]}'
          if ! aws iam get-role --role-name "$LAMBDA_ROLE" >/dev/null 2>&1; then
            aws iam create-role --role-name "$LAMBDA_ROLE" --assume-role-policy-document "$TRUST" >/dev/null
          fi
          aws iam attach-role-policy --role-name "$LAMBDA_ROLE" --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole >/dev/null 2>&1 || true
          aws iam attach-role-policy --role-name "$LAMBDA_ROLE" --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaVPCAccessExecutionRole >/dev/null 2>&1 || true
          aws iam attach-role-policy --role-name "$LAMBDA_ROLE" --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaSQSQueueExecutionRole >/dev/null 2>&1 || true
          # Дозвіл інжест-ламбді відправляти в SQS (custom policy)
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          cat > sqs-send.json <<EOF
          {"Version":"2012-10-17","Statement":[{"Effect":"Allow","Action":["sqs:SendMessage"],"Resource":"$QUEUE_ARN"}]}
          EOF
          if ! aws iam get-policy --policy-arn arn:aws:iam::$ACCOUNT_ID:policy/iotlab-sqs-send >/dev/null 2>&1; then
            aws iam create-policy --policy-name iotlab-sqs-send --policy-document file://sqs-send.json >/dev/null
          fi
          aws iam attach-role-policy --role-name "$LAMBDA_ROLE" --policy-arn arn:aws:iam::$ACCOUNT_ID:policy/iotlab-sqs-send >/dev/null 2>&1 || true
          ROLE_ARN=$(aws iam get-role --role-name "$LAMBDA_ROLE" --query 'Role.Arn' --output text)
          echo "ROLE_ARN=$ROLE_ARN" >> $GITHUB_ENV
      - name: Build & deploy Lambdas (ingest -> SQS, consumer SQS -> MySQL)
        run: |
          set -e
          wait_lambda_ok () {
            FN="$1"
            for i in $(seq 1 60); do
              STATUS=$(aws lambda get-function-configuration --function-name "$FN" --query 'LastUpdateStatus' --output text 2>/dev/null || echo "Unknown")
              if [ "$STATUS" = "Successful" ] || [ "$STATUS" = "Unknown" ]; then
                break
              fi
              if [ "$STATUS" = "Failed" ]; then
                echo "Lambda $FN last update FAILED"; exit 1
              fi
              sleep 3
            done
          }
          mkdir -p lambda_ingest lambda_consumer
          # -------- ingest (API -> SQS)
          cat > lambda_ingest/handler.py <<'PY'
          import os, json, boto3, time
          sqs = boto3.client('sqs')
          QUEUE_URL = os.environ['QUEUE_URL']
          def lambda_handler(event, context):
              if event.get("isBase64Encoded"):
                  body = json.loads(bytearray(event["body"], "utf-8").decode())
              else:
                  body = json.loads(event.get("body") or "{}")
              body.setdefault("ts", int(time.time()*1000))
              sqs.send_message(QueueUrl=QUEUE_URL, MessageBody=json.dumps(body))
              return {"statusCode": 202, "headers":{"Content-Type":"application/json"}, "body": json.dumps({"ok": True})}
          PY
          # -------- consumer (SQS -> MySQL)
          cat > lambda_consumer/handler.py <<'PY'
          import os, json, pymysql, time
          DB_HOST = os.environ['DB_HOST']
          DB_NAME = os.environ['DB_NAME']
          DB_USER = os.environ['DB_USER']
          DB_PASS = os.environ['DB_PASS']
          def _conn():
              return pymysql.connect(host=DB_HOST, user=DB_USER, password=DB_PASS, database=DB_NAME, autocommit=True, charset="utf8mb4")
          def _init_schema(cur):
              cur.execute("""
              CREATE TABLE IF NOT EXISTS sensor_data (
                id BIGINT AUTO_INCREMENT PRIMARY KEY,
                sensor_type VARCHAR(32) NOT NULL,
                value DOUBLE NOT NULL,
                lat DOUBLE NULL,
                lon DOUBLE NULL,
                ts BIGINT NOT NULL
              ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
              """)
          def lambda_handler(event, context):
              con = _conn()
              try:
                  with con.cursor() as cur:
                      _init_schema(cur)
                      for rec in event.get('Records', []):
                          payload = json.loads(rec['body'])
                          cur.execute(
                            "INSERT INTO sensor_data(sensor_type, value, lat, lon, ts) VALUES(%s,%s,%s,%s,%s)",
                            (
                              payload.get('type'),
                              float(payload.get('value', 0)),
                              payload.get('lat'),
                              payload.get('lon'),
                              int(payload.get('ts', time.time()*1000))
                            )
                          )
              finally:
                  con.close()
              return {"ok": True}
          PY
          # Vendoring deps for consumer
          python -m pip install --upgrade pip >/dev/null
          pip install -q -t lambda_consumer/python pymysql
          (cd lambda_ingest && zip -rq ../ingest.zip .)
          (cd lambda_consumer && zip -rq ../consumer_layer.zip python)
          (cd lambda_consumer && zip -rq ../consumer_code.zip handler.py)
          # Create/Update consumer layer
          LAYER_ARN=$(aws lambda publish-layer-version --layer-name iotlab-pymysql --zip-file fileb://consumer_layer.zip --compatible-runtimes python3.12 --query 'LayerVersionArn' --output text)
          # Create/Update ingest function
          if aws lambda get-function --function-name "$INGEST_LAMBDA" >/dev/null 2>&1; then
            aws lambda update-function-code --function-name "$INGEST_LAMBDA" --zip-file fileb://ingest.zip >/dev/null
            wait_lambda_ok "$INGEST_LAMBDA"
          else
            aws lambda create-function --function-name "$INGEST_LAMBDA" \
              --runtime python3.12 --handler handler.lambda_handler \
              --role "$ROLE_ARN" --timeout 15 --memory-size 128 \
              --zip-file fileb://ingest.zip \
              --environment "Variables={QUEUE_URL=$QUEUE_URL}" >/dev/null
            wait_lambda_ok "$INGEST_LAMBDA"
          fi
          # Create/Update consumer function (in VPC)
          if aws lambda get-function --function-name "$CONSUMER_LAMBDA" >/dev/null 2>&1; then
            aws lambda update-function-code --function-name "$CONSUMER_LAMBDA" --zip-file fileb://consumer_code.zip >/dev/null
            wait_lambda_ok "$CONSUMER_LAMBDA"
            aws lambda update-function-configuration --function-name "$CONSUMER_LAMBDA" \
              --layers "$LAYER_ARN" \
              --vpc-config "SubnetIds=$SUBNET1,$SUBNET2,SecurityGroupIds=$LAMBDA_SG_ID" \
              --environment "Variables={DB_HOST=$DB_HOST,DB_NAME=$DB_NAME,DB_USER=$DB_USER,DB_PASS=$DB_MASTER_PASS}" >/dev/null
            wait_lambda_ok "$CONSUMER_LAMBDA"
          else
            aws lambda create-function --function-name "$CONSUMER_LAMBDA" \
              --runtime python3.12 --handler handler.lambda_handler \
              --role "$ROLE_ARN" --timeout 30 --memory-size 256 \
              --zip-file fileb://consumer_code.zip \
              --layers "$LAYER_ARN" \
              --vpc-config "SubnetIds=$SUBNET1,$SUBNET2,SecurityGroupIds=$LAMBDA_SG_ID" \
              --environment "Variables={DB_HOST=$DB_HOST,DB_NAME=$DB_NAME,DB_USER=$DB_USER,DB_PASS=$DB_MASTER_PASS}" >/dev/null
            wait_lambda_ok "$CONSUMER_LAMBDA"
          fi
          # Event source mapping SQS -> consumer
          MAPPING_ID=$(aws lambda list-event-source-mappings --function-name "$CONSUMER_LAMBDA" --query "EventSourceMappings[?EventSourceArn=='$QUEUE_ARN'].UUID | [0]" --output text)
          if [ -z "$MAPPING_ID" ] || [ "$MAPPING_ID" = "None" ]; then
            aws lambda create-event-source-mapping --function-name "$CONSUMER_LAMBDA" --event-source-arn "$QUEUE_ARN" --batch-size 10 >/dev/null
          fi
      - name: API Gateway for ingest Lambda
        id: apigw
        run: |
          set -e
          # Create/lookup REST API
          API_ID=$(aws apigateway get-rest-apis --query "items[?name=='$API_NAME'].id | [0]" --output text)
          if [ "$API_ID" = "None" ] || [ -z "$API_ID" ]; then
            API_ID=$(aws apigateway create-rest-api --name "$API_NAME" --region "$AWS_REGION" --query 'id' --output text)
          fi
          ROOT_ID=$(aws apigateway get-resources --rest-api-id "$API_ID" --query "items[?path=='/'].id" --output text)
          # /ingest resource + POST method -> Lambda proxy
          RES_ID=$(aws apigateway get-resources --rest-api-id "$API_ID" --query "items[?path=='/ingest'].id" --output text)
          if [ "$RES_ID" = "None" ] || [ -z "$RES_ID" ]; then
            RES_ID=$(aws apigateway create-resource --rest-api-id "$API_ID" --parent-id "$ROOT_ID" --path-part ingest --query 'id' --output text)
          fi
          aws apigateway put-method --rest-api-id "$API_ID" --resource-id "$RES_ID" --http-method POST --authorization-type "NONE" >/dev/null 2>&1 || true
          LAMBDA_ARN=$(aws lambda get-function --function-name "$INGEST_LAMBDA" --query 'Configuration.FunctionArn' --output text)
          URI="arn:aws:apigateway:${AWS_REGION}:lambda:path/2015-03-31/functions/${LAMBDA_ARN}/invocations"
          aws apigateway put-integration --rest-api-id "$API_ID" --resource-id "$RES_ID" --http-method POST \
            --type AWS_PROXY --integration-http-method POST --uri "$URI" >/dev/null 2>&1 || true
          # permission for API to call lambda
          aws lambda add-permission --function-name "$INGEST_LAMBDA" --statement-id "apigw-post-ingest" \
            --action lambda:InvokeFunction --principal apigateway.amazonaws.com \
            --source-arn "arn:aws:execute-api:${AWS_REGION}:$(aws sts get-caller-identity --query Account --output text):${API_ID}/*/POST/ingest" >/dev/null 2>&1 || true
          # Deploy
          aws apigateway create-deployment --rest-api-id "$API_ID" --stage-name "$STAGE" >/dev/null
          API_URL="https://${API_ID}.execute-api.${AWS_REGION}.amazonaws.com/${STAGE}"
          echo "API_URL=$API_URL" >> $GITHUB_ENV
          echo "API URL: $API_URL/ingest"
      - name: Ensure ECR repo
        run: |
          set -e
          aws ecr describe-repositories --repository-names "$ECR_REPOSITORY" >/dev/null 2>&1 || \
            aws ecr create-repository --repository-name "$ECR_REPOSITORY" >/dev/null
      - name: Login to ECR
        id: ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Build & push emulator
        env:
          ECR_REGISTRY: ${{ steps.ecr.outputs.registry }}
        run: |
          set -e
          mkdir -p emu
          cat > emu/Dockerfile <<'DOCKER'
          FROM python:3.11-slim
          WORKDIR /app
          RUN pip install --no-cache-dir requests
          COPY run.py .
          CMD ["python","run.py"]
          DOCKER
          cat > emu/run.py <<'PY'
          import os, time, json, random, threading, requests
          API = os.getenv("TARGET", "") + "/ingest"
          TEMP_MS = int(os.getenv("TEMP_MS","50"))
          HUM_MS  = int(os.getenv("HUM_MS","80"))
          LUX_MS  = int(os.getenv("LUX_MS","100"))
          def loop(name, period_ms, gen):
              while True:
                  payload = {"type": name, "value": gen(), "lat": 49.84, "lon": 24.03}
                  try:
                      requests.post(API, data=json.dumps(payload), timeout=5)
                  except Exception:
                      pass
                  time.sleep(max(0.02, period_ms/1000.0))
          threads = [
              threading.Thread(target=loop, args=("temperature", TEMP_MS, lambda: round(random.uniform(18, 28), 2)), daemon=True),
              threading.Thread(target=loop, args=("humidity", HUM_MS,    lambda: round(random.uniform(40, 70), 1)), daemon=True),
              threading.Thread(target=loop, args=("light", LUX_MS,       lambda: round(random.uniform(100, 800), 0)), daemon=True),
          ]
          [t.start() for t in threads]
          while True: time.sleep(3600)
          PY
          docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:latest emu
          docker push $ECR_REGISTRY/$ECR_REPOSITORY:latest
          echo "EMU_IMAGE=$ECR_REGISTRY/$ECR_REPOSITORY:latest" >> $GITHUB_ENV
      - name: Run emulator once on Fargate
        run: |
          set -e
          aws ecs create-cluster --cluster-name "$CLUSTER_NAME" >/dev/null 2>&1 || true
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          cat > taskdef.json <<JSON
          {
            "family": "${EMU_TASK_FAMILY}",
            "networkMode": "awsvpc",
            "requiresCompatibilities": ["FARGATE"],
            "cpu": "${EMU_CPU}",
            "memory": "${EMU_MEM}",
            "executionRoleArn": "arn:aws:iam::${ACCOUNT_ID}:role/ecsTaskExecutionRole",
            "taskRoleArn": "arn:aws:iam::${ACCOUNT_ID}:role/ecsTaskExecutionRole",
            "containerDefinitions": [{
              "name": "${EMU_CONTAINER}",
              "image": "${EMU_IMAGE}",
              "essential": true,
              "environment": [
                {"name":"TARGET","value":"${API_URL}"},
                {"name":"TEMP_MS","value":"${TEMP_PERIOD_MS}"},
                {"name":"HUM_MS","value":"${HUM_PERIOD_MS}"},
                {"name":"LUX_MS","value":"${LIGHT_PERIOD_MS}"}
              ]
            }]
          }
          JSON
          # ensure task exec role
          if ! aws iam get-role --role-name ecsTaskExecutionRole >/dev/null 2>&1; then
            printf '%s' '{"Version":"2012-10-17","Statement":[{"Effect":"Allow","Principal":{"Service":"ecs-tasks.amazonaws.com"},"Action":"sts:AssumeRole"}]}' > trust.json
            aws iam create-role --role-name ecsTaskExecutionRole --assume-role-policy-document file://trust.json >/dev/null
            aws iam attach-role-policy --role-name ecsTaskExecutionRole --policy-arn arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy >/dev/null
          fi
          aws ecs register-task-definition --cli-input-json file://taskdef.json >/dev/null
          aws ecs run-task \
            --launch-type FARGATE \
            --cluster "$CLUSTER_NAME" \
            --task-definition "$EMU_TASK_FAMILY" \
            --network-configuration "awsvpcConfiguration={subnets=[$SUBNET1,$SUBNET2],assignPublicIp=ENABLED}" >/dev/null
          echo "Emulator started → ${API_URL}/ingest"